---
title: "Lab 2"
author: 'IMC 490: Machine Learning for IMC'
date: "April 5, 2017"
output: pdf_document
---

```{r, message = F, echo = F}
require(knitr)
knitr::opts_chunk$set(fig.width = 5, fig.height = 3.75)
```


In this lab, we'll be going over the following topics:

- installing and loading packages
- fitting a linear regression model
- inspecting and visualizing a linear regression model and its diagnostics
- fitting a multiple regression model
- making predictions

Remember to use `?command` or `help(command)` in the R console to access documentation at any time if you have questions.

### Setup
`install.packages()`  
`library()`  
`require()`  

To get started, let's install and load the "ISLR" package, which contains the datasets used in the textbook.

```{r, eval=F}
# you can install any package with install.packages("package")
install.packages("ISLR")
```

```{r}
# load the package
library(ISLR)
```

`library()` and `require()` both load packages - you can use either one.

\newpage

### Simple Linear Regression
`lm()`  
`summmary()`  
`plot()`  
`predict()`  

Let's start by loading the Auto data from last time, but using the "ISLR" package this time.

```{r}
data("Auto")
```

**Our task is to predict the mpg (miles per gallon) of a vehicle.** Always begin each analysis task by exploring the data using `names()` and `str()`.  

```{r}
names(Auto)
```

**Now let's fit a single variable regression using weight to predict mpg.** To fit a regression, we pass the regression equation into the `lm()` (linear model) function. Note that because the equality operator `=` is reserved for assignment in R, we must use the `~` operator in place of `=` to define the regression equation. Our regression equation is:

$$mpg = \beta_0 + \beta_1*weight$$
This equation, translated into R code, would be: `mpg ~ weight`. *Note that the intercept ($\beta_0$) is implicit.* Now let's fit the model by passing in the equation and dataset into the `lm()` function, then inspect it using `summary()`.

```{r}
lm_fit_1 = lm(mpg ~ weight, data = Auto)
summary(lm_fit_1)
```

See "Anatomy of a Regression Summary" on Canvas (Labs -> Lab 2 - regression -> Regression Summary.pdf) for a visual explanation of this summary.

\newpage

### Visualizing the linear relationship

`plot()`  
`abline()`  

To visualize the linear relationship between two predictors, first create a scatterplot, then pass the regression into `abline()` to draw the line. You can set the parameters `lwd` (line weight) and `col` (color) to something more aesthetically pleasing.

```{r}
plot(Auto$weight, Auto$mpg)
abline(lm_fit_1, lwd = 3, col = "red")
```

\newpage

### Diagnostic plots

When training regression models, it is very important to test for model misspecification. *Recall that an assumption of linear regression is independent and identically (normally) distributed residuals.*. To cycle through diagnostic plots, pass your model into the `plot()` function. To grab a specific plot, specify a number after the regression model.

**Residuals plot: Checks for randomly distributed residuals. This plot should be a random "snowstorm" of residuals.** However, in this example we see a clear increase in residual variance across the x axis. This clues us in on the presence of heteroschedasticity, or non-constant variance. 

```{r, fig.height=3.6}
plot(lm_fit_1, 1)
```

**Q-Q plot: Checks for normality of residuals. Perfectly normal residuals should result in a 45 degree line.** Since the residuals diverge from the 45 degree line, we conclude that the residuals do not satisfy the normality requirement very well.

```{r, fig.height=3.6}
plot(lm_fit_1, 2)
```

**Let's see what happens when you take a variance stabilizing transformation of mpg.** 

We'll use `log(mpg)`

```{r}
lm_fit_2 = lm(log(mpg) ~ weight, Auto)  
plot(lm_fit_2, 1)
plot(lm_fit_2, 2)
```



\newpage

### Multiple Regression
`predict()`

**Now let's train a regression with multiple predictors.** To regress on multiple predictors, simply define the regression equation with additional predictors using the addition operator.

```{r}
lm_fit_3 = lm(mpg ~ weight + horsepower + year, data = Auto)
lm_fit_3
```


Tip: you can train on all the predictors by putting a dot (.) on the right side of the regression equation instead of variable names.

```{r}
lm_fit_4 = lm(mpg ~ ., data = Auto[ ,-9])
lm_fit_4
```

Finally, to make predictions using your model, pass the model and your new data into the `predict()` function. Let's compare the predictions using our simple regression model versus the multiple regression model.

```{r}
Auto$mpg[10:12]
predict(lm_fit_1, Auto[10:12, ])
predict(lm_fit_4, Auto[10:12, ])
```

Our multiple regression model is more accurate than our simple regression model, predicting values closer to the true `mpg`.

\newpage

### Analysis of correlation
`cor()`  
`car::vif()`

Now that we've begun using libraries/packages, we will introduce the `::` operator. `::` allows you to check which functions belong to a package. For instance, our function for calculating variance inflation factor `vif()` is a part of the "car" package. To use this function, we can either load the entire library using `library(car)` and then use `vif(lm_fit)`, or we can use the `::` operator to pull the function directly `car::vif(lm_fit)`.   

When performing regression using multiple predictors, it is important to check for correlation between the predictors, or multicollinearity.  

Correlation matrix:

```{r}
cor(Auto[ ,-9])
```

Variance inflation factor:

```{r}
library(car)
vif(lm_fit_4)
```






















